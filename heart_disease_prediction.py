# -*- coding: utf-8 -*-
"""Heart_disease_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hzwb8C99mzszXBx-ULuJpZQd3TUd3ePT

## <font size=5> <strong>Heart Disease Prediction
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv("D:\Ai\heart.csv")

dataset.shape

dataset.head(5)

dataset.sample(5)

dataset.info()

#condition
info = ["age","1: male, 0: female","chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic","resting blood pressure"," serum cholestoral in mg/dl","fasting blood sugar > 120 mg/dl","resting electrocardiographic results (values 0,1,2)"," maximum heart rate achieved","exercise induced angina","oldpeak = ST depression induced by exercise relative to rest","the slope of the peak exercise ST segment","number of major vessels (0-3) colored by flourosopy","thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"]


for i in range(len(info)):
    print(dataset.columns[i]+":\t\t\t"+info[i])

dataset["target"].describe()

dataset["target"].unique()

"""### Correlation between columns"""

print(dataset.corr()["target"].sort_values(ascending=False))

"""## Data Preprocessing"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']


preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
    ])
predictors = dataset.drop("target",axis=1)

predictors_processed = preprocessor.fit_transform(predictors)
dataset

dataset.isna()

dataset.isna().sum()

"""## Exploratory Data Analysis (EDA)"""

y = dataset["target"]

sns.countplot(x = y)


target_temp = dataset.target.value_counts()

print(target_temp)

print("Percentage of patience without heart problems: "+str(round(target_temp[0]*100/303,2)))
print("Percentage of patience with heart problems: "+str(round(target_temp[1]*100/303,2)))

dataset["sex"].unique()

sns.countplot(x = dataset["sex"])

"""### Analysing the Chest Pain Type feature"""

dataset["cp"].unique()

sns.countplot(x=dataset["cp"])

"""### Analysing the Fasting Blood Sugar feature"""

dataset["fbs"].describe()

dataset["fbs"].unique()

sns.countplot(x = dataset["fbs"])

"""### Analysing the resting electrocardiographic feature"""

dataset["restecg"].unique()

sns.countplot(x = dataset["restecg"])

"""### Analysing the exercise induced angina feature"""

dataset["exang"].unique()

sns.countplot(x  = dataset["exang"])

"""### Analysing the Slope feature"""

dataset["slope"].unique()

sns.countplot(x = dataset["slope"])

"""### Analysing the number of major vessels (0-3) feature"""

dataset["ca"].unique()

sns.countplot(x = dataset["ca"])

"""##### ca=4 has astonishingly large number of heart patients

### Analysing the 'thal' feature
"""

dataset["thal"].unique()

sns.countplot( x = dataset["thal"])

sns.histplot(dataset["thal"])

"""## IV. Train Test split"""

from sklearn.model_selection import train_test_split

predictors = dataset.drop("target",axis=1)
target = dataset["target"]

X_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20)

X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

"""## V. Model Fitting"""

from sklearn.metrics import accuracy_score

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

lr.fit(X_train,Y_train)

Y_pred_lr = lr.predict(X_test)

Y_pred_lr.shape

score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)

print("The accuracy score achieved using Logistic Regression is: "+str(score_lr)+" %")

"""### Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

nb.fit(X_train,Y_train)

Y_pred_nb = nb.predict(X_test)

Y_pred_nb.shape

score_nb = round(accuracy_score(Y_pred_nb,Y_test)*100,2)

print("The accuracy score achieved using Naive Bayes is: "+str(score_nb)+" %")

"""### SVM"""

from sklearn import svm

sv = svm.SVC(kernel='linear')

sv.fit(X_train, Y_train)

Y_pred_svm = sv.predict(X_test)

Y_pred_svm.shape

score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)

print("The accuracy score achieved using Linear SVM is: "+str(score_svm)+" %")

"""### K Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,Y_train)
Y_pred_knn=knn.predict(X_test)

Y_pred_knn.shape

score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)

print("The accuracy score achieved using KNN is: "+str(score_knn)+" %")

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

max_accuracy = 0


for x in range(200):
    dt = DecisionTreeClassifier(random_state=x)
    dt.fit(X_train,Y_train)
    Y_pred_dt = dt.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x

#print(max_accuracy)
#print(best_x)


dt = DecisionTreeClassifier(random_state=best_x)
dt.fit(X_train,Y_train)
Y_pred_dt = dt.predict(X_test)

print(Y_pred_dt.shape)

score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)

print("The accuracy score achieved using Decision Tree is: "+str(score_dt)+" %")

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

max_accuracy = 0


for x in range(2000):
    rf = RandomForestClassifier(random_state=x)
    rf.fit(X_train,Y_train)
    Y_pred_rf = rf.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x

#print(max_accuracy)
#print(best_x)

rf = RandomForestClassifier(random_state=best_x)
rf.fit(X_train,Y_train)
Y_pred_rf = rf.predict(X_test)

Y_pred_rf.shape

score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)

print("The accuracy score achieved using Decision Tree is: "+str(score_rf)+" %")

"""## VI. final"""

scores = [score_lr,score_nb,score_svm,score_knn,score_dt,score_rf]
algorithms = ["Logistic Regression","Naive Bayes","Support Vector Machine","K-Nearest Neighbors","Decision Tree","Random Forest"]

for i in range(len(algorithms)):
    print("The accuracy score achieved using "+algorithms[i]+" is: "+str(scores[i])+" %")

sns.set(rc={'figure.figsize':(15,8)})
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")

sns.barplot(x = algorithms, y = scores)

"""# Testing

## Logistic  Regression
"""

from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score,
)
score_lr = round(accuracy_score(Y_test, Y_pred_lr) * 100, 2)
print("The accuracy score achieved using Logistic Regression is: " + str(score_lr) + " %")

cm = confusion_matrix(Y_test, Y_pred_lr)
print("Confusion Matrix:\n", cm)

precision = precision_score(Y_test, Y_pred_lr)
print("Precision: {:.2f}".format(precision))

recall = recall_score(Y_test, Y_pred_lr)
print("Recall: {:.2f}".format(recall))

f1 = f1_score(Y_test, Y_pred_lr)
print("F1 Score: {:.2f}".format(f1))

tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
print("Specificity: {:.2f}".format(specificity))

"""## Naive Bayes"""

score_nb = round(accuracy_score(Y_test, Y_pred_nb) * 100, 2)
print("The accuracy score achieved using Naive Bayes is: " + str(score_nb) + " %")

cm_nb = confusion_matrix(Y_test, Y_pred_nb)
print("Confusion Matrix for Naive Bayes:\n", cm_nb)

precision_nb = precision_score(Y_test, Y_pred_nb)
print("Precision for Naive Bayes: {:.2f}".format(precision_nb))

recall_nb = recall_score(Y_test, Y_pred_nb)
print("Recall for Naive Bayes: {:.2f}".format(recall_nb))

f1_nb = f1_score(Y_test, Y_pred_nb)
print("F1 Score for Naive Bayes: {:.2f}".format(f1_nb))

tn_nb, fp_nb, fn_nb, tp_nb = cm_nb.ravel()
specificity_nb = tn_nb / (tn_nb + fp_nb)
print("Specificity for Naive Bayes: {:.2f}".format(specificity_nb))

"""##  SVM"""

score_svm = round(accuracy_score(Y_test, Y_pred_svm) * 100, 2)
print("The accuracy score achieved using SVM is: " + str(score_svm) + " %")

cm_svm = confusion_matrix(Y_test, Y_pred_svm)
print("Confusion Matrix for SVM:\n", cm_svm)

precision_svm = precision_score(Y_test, Y_pred_svm)
print("Precision for SVM: {:.2f}".format(precision_svm))

recall_svm = recall_score(Y_test, Y_pred_svm)
print("Recall for SVM: {:.2f}".format(recall_svm))

f1_svm = f1_score(Y_test, Y_pred_svm)
print("F1 Score for SVM: {:.2f}".format(f1_svm))

tn_svm, fp_svm, fn_svm, tp_svm = cm_svm.ravel()
specificity_svm = tn_svm / (tn_svm + fp_svm)
print("Specificity for SVM: {:.2f}".format(specificity_svm))

score_knn = round(accuracy_score(Y_test, Y_pred_knn) * 100, 2)
print("The accuracy score achieved using KNN is: " + str(score_knn) + " %")

cm_knn = confusion_matrix(Y_test, Y_pred_knn)
print("Confusion Matrix for KNN:\n", cm_knn)

precision_knn = precision_score(Y_test, Y_pred_knn)
print("Precision for KNN: {:.2f}".format(precision_knn))

recall_knn = recall_score(Y_test, Y_pred_knn)
print("Recall for KNN: {:.2f}".format(recall_knn))

f1_knn = f1_score(Y_test, Y_pred_knn)
print("F1 Score for KNN: {:.2f}".format(f1_knn))

tn_knn, fp_knn, fn_knn, tp_knn = cm_knn.ravel()
specificity_knn = tn_knn / (tn_knn + fp_knn)
print("Specificity for KNN: {:.2f}".format(specificity_knn))

"""## Decision Tree"""

score_dt = round(accuracy_score(Y_test, Y_pred_dt) * 100, 2)
print("The accuracy score achieved using Decision Tree is: " + str(score_dt) + " %")

cm_dt = confusion_matrix(Y_test, Y_pred_dt)
print("Confusion Matrix for Decision Tree:\n", cm_dt)

precision_dt = precision_score(Y_test, Y_pred_dt)
print("Precision for Decision Tree: {:.2f}".format(precision_dt))

recall_dt = recall_score(Y_test, Y_pred_dt)
print("Recall for Decision Tree: {:.2f}".format(recall_dt))

f1_dt = f1_score(Y_test, Y_pred_dt)
print("F1 Score for Decision Tree: {:.2f}".format(f1_dt))

tn_dt, fp_dt, fn_dt, tp_dt = cm_dt.ravel()
specificity_dt = tn_dt / (tn_dt + fp_dt)
print("Specificity for Decision Tree: {:.2f}".format(specificity_dt))

"""##  Random Forest"""

score_rf = round(accuracy_score(Y_test, Y_pred_rf) * 100, 2)
print("The accuracy score achieved using Random Forest is: " + str(score_rf) + " %")

cm_rf = confusion_matrix(Y_test, Y_pred_rf)
print("Confusion Matrix for Random Forest:\n", cm_rf)

precision_rf = precision_score(Y_test, Y_pred_rf)
print("Precision for Random Forest: {:.2f}".format(precision_rf))

recall_rf = recall_score(Y_test, Y_pred_rf)
print("Recall for Random Forest: {:.2f}".format(recall_rf))

f1_rf = f1_score(Y_test, Y_pred_rf)
print("F1 Score for Random Forest: {:.2f}".format(f1_rf))

tn_rf, fp_rf, fn_rf, tp_rf = cm_rf.ravel()
specificity_rf = tn_rf / (tn_rf + fp_rf)
print("Specificity for Random Forest: {:.2f}".format(specificity_rf))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score

models = ["Logistic Regression", "Naive Bayes", "SVM", "KNN", "Decision Tree", "Random Forest"]

results = []

Y_pred_models = [Y_pred_lr, Y_pred_nb, Y_pred_svm, Y_pred_knn, Y_pred_dt, Y_pred_rf]

for model, Y_pred in zip(models, Y_pred_models):
    accuracy = round(accuracy_score(Y_test, Y_pred) * 100, 2)
    cm = confusion_matrix(Y_test, Y_pred)
    precision = round(precision_score(Y_test, Y_pred), 2)
    recall = round(recall_score(Y_test, Y_pred), 2)
    f1 = round(f1_score(Y_test, Y_pred), 2)


    tn, fp, fn, tp = cm.ravel()
    specificity = round(tn / (tn + fp), 2)

    results.append([model, accuracy, precision, recall, f1, specificity])


metrics_df = pd.DataFrame(results, columns=["Model", "Accuracy", "Precision", "Recall", "F1 Score", "Specificity"])

print(metrics_df)

"""## Accuracy

"""

metrics_to_plot = ["Accuracy", "Precision", "Recall", "F1 Score", "Specificity"]


sns.set(rc={'figure.figsize':(10,6)})

plt.figure()
sns.barplot(x='Model', y='Accuracy', data=metrics_df)
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.title('Comparison of Accuracy')
plt.show()

"""## Precision"""

plt.figure()
sns.barplot(x='Model', y='Precision', data=metrics_df)
plt.xlabel('Algorithms')
plt.ylabel('Precision')
plt.title('Comparison of Precision')
plt.show()

"""## Recall

"""

plt.figure()
sns.barplot(x='Model', y='Recall', data=metrics_df)
plt.xlabel('Algorithms')
plt.ylabel('Recall')
plt.title('Comparison of Recall')
plt.show()

"""## F1 Score"""

plt.figure()
sns.barplot(x='Model', y='F1 Score', data=metrics_df)
plt.xlabel('Algorithms')
plt.ylabel('F1 Score')
plt.title('Comparison of F1 Score')
plt.show()

"""## Specificity

"""

plt.figure()
sns.barplot(x='Model', y='Specificity', data=metrics_df)
plt.xlabel('Algorithms')
plt.ylabel('Specificity')
plt.title('Comparison of Specificity')
plt.show()

from sklearn.metrics import accuracy_score

models = [
    ("Logistic Regression", lr),
    ("Naive Bayes", nb),
    ("SVM", sv),
    ("KNN", knn),
    ("Decision Tree", dt),
    ("Random Forest", rf),
]


train_accuracies = []
test_accuracies = []



for name, model in models:
    # Training
    Y_train_pred = model.predict(X_train)
    train_accuracy = round(accuracy_score(Y_train, Y_train_pred) * 100, 2)
    train_accuracies.append(train_accuracy)

    # Test
    Y_test_pred = model.predict(X_test)
    test_accuracy = round(accuracy_score(Y_test, Y_test_pred) * 100, 2)
    test_accuracies.append(test_accuracy)

model_names = []
for name, _ in models:
    model_names.append(name)

accuracy_df = pd.DataFrame({
    'Model': model_names,
    'Training Accuracy': train_accuracies,
    'Test Accuracy': test_accuracies
})

print(accuracy_df)


plt.figure()
sns.barplot(x='Model', y='value', hue='variable',
            data=pd.melt(accuracy_df, id_vars=['Model'], value_vars=['Training Accuracy', 'Test Accuracy']))
plt.title('Training vs Test Accuracy Comparison')
plt.ylabel('Accuracy (%)')
plt.show()

import pandas as pd

print("Please provide the following values:")

age = int(input("Enter age : "))
sex = int(input("Enter sex (1 = male, 0 = female): "))
cp = int(input("Enter chest pain type (0 = typical angina, 1 = atypical angina, 2 = non-anginal pain, 3 = asymptomatic): "))
trestbps = int(input("Enter resting blood pressure in mm Hg (Ideal range: 94-200): "))
chol = int(input("Enter serum cholesterol in mg/dL (Ideal range: 126-564): "))
fbs = int(input("Is fasting blood sugar > 120 mg/dL? (1 = yes, 0 = no): "))
restecg = int(input("Enter resting electrocardiographic results (0 = normal, 1 = ST-T wave abnormality, 2 = left ventricular hypertrophy): "))
thalach = int(input("Enter maximum heart rate achieved (Ideal range: 71-202): "))
exang = int(input("Enter exercise-induced angina (1 = yes, 0 = no): "))
oldpeak = float(input("Enter ST depression induced by exercise relative to rest (Ideal range: 0.0-6.2): "))
slope = int(input("Enter the slope of the peak exercise ST segment (0 = upsloping, 1 = flat, 2 = downsloping): "))
ca = int(input("Enter number of major vessels (0-3) colored by fluoroscopy: "))
thal = int(input("Enter thalassemia type (1 = normal, 2 = fixed defect, 3 = reversible defect): "))


custom_data = {
    'age': age,
    'sex': sex,
    'cp': cp,
    'trestbps': trestbps,
    'chol': chol,
    'fbs': fbs,
    'restecg': restecg,
    'thalach': thalach,
    'exang': exang,
    'oldpeak': oldpeak,
    'slope': slope,
    'ca': ca,
    'thal': thal
}

custom_df = pd.DataFrame([custom_data])

custom_prediction = rf.predict(custom_df)
custom_probability = rf.predict_proba(custom_df)[:, 1]

print(f"\nYou Have : {'High Chance of Heart Disease' if custom_probability[0] > 0.5 else 'Low Chance of Heart Disease'}")
print(f"Probability of Heart Disease: {custom_probability[0] * 100:.2f}%")

